import os
import json
from pathlib import Path
from datetime import datetime

from dotenv import load_dotenv
from openai import OpenAI

# ---------- Paths ----------
BASE = Path(__file__).resolve().parent
BRAIN = BASE / "brain.md"
RULES = BASE / "rules.md"
WORKFLOWS_DIR = BASE / "workflows"
MEMORY_DIR = BASE / "memory"
STATE_FILE = MEMORY_DIR / "state.json"

# ---------- Helpers ----------
def read_text(p: Path) -> str:
    return p.read_text(encoding="utf-8") if p.exists() else ""

def load_workflows(dir_path: Path) -> str:
    if not dir_path.exists():
        return ""
    parts = []
    for p in sorted(dir_path.rglob("*.md")):
        parts.append(f"\n\n# WORKFLOW: {p.relative_to(dir_path)}\n{read_text(p)}")
    return "\n".join(parts).strip()

def load_state() -> dict:
    MEMORY_DIR.mkdir(parents=True, exist_ok=True)
    if STATE_FILE.exists():
        try:
            return json.loads(STATE_FILE.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_state(state: dict) -> None:
    MEMORY_DIR.mkdir(parents=True, exist_ok=True)
    STATE_FILE.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")

def now_iso() -> str:
    return datetime.now().isoformat(timespec="seconds")

# ---------- Main ----------
def main():
    load_dotenv(BASE / ".env")
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    model = os.getenv("BEZEN_MODEL", "gpt-4.1-mini").strip()

    if not api_key or api_key.startswith("PASTE_"):
        print("âŒ ×—×¡×¨ OPENAI_API_KEY. ×¤×ª×— ××ª agents/BEZEN-CORE/.env ×•×”×“×‘×§ ××¤×ª×—.")
        return

    client = OpenAI(api_key=api_key)

    brain = read_text(BRAIN)
    rules = read_text(RULES)
    workflows = load_workflows(WORKFLOWS_DIR)

    state = load_state()
    history = state.get("history", [])

    system_prompt = f"""
××ª×” BEZEN-CORE â€” ×¡×•×›×Ÿ ××•×˜×•× ×•××™ ×©××˜×¨×ª×•:
1) ×œ×–×”×•×ª "××©×¤×˜ ×˜×¢×•×Ÿ ×©×™×¤×•×¨" ××ª×•×š ×“×‘×¨×™ ×”××©×ª××©
2) ×œ×¤×¨×§ ××•×ª×• ×œ×©×›×‘×•×ª (×—×¡×/×××•× ×”/×¨×’×©/×ª×—×•×©×”/×“×¤×•×¡ ×—×©×™×‘×”/×”×ª× ×”×’×•×ª)
3) ×œ×”×¦×™×¢ ××©×¤×˜ ××¢×‘×¨
4) ×œ×”×¦×™×¢ ××©×¤×˜ ××•×¢×™×œ
5) ×œ×”×¦×™×¢ ×ª×›×•× ×” ××•×¢×™×œ×” ×××•×–× ×ª + ××©×™××” ×§×˜× ×” ××—×ª (10-60 ×©× ×™×•×ª) ×›×“×™ ×œ×™×¦×•×¨ "×©×œ×™×˜×” ×§×˜× ×”"
6) ×œ×©××•×¨ ×¢×§×‘×™×•×ª ×¢× brain/rules/workflows

×¢×§×¨×•× ×•×ª:
- ×¢×‘×¨×™×ª ×˜×‘×¢×™×ª, ×§×¦×¨×”, ×‘×œ×™ ×—×¤×™×¨×•×ª.
- ×©××œ×” ××—×ª ×‘×›×œ ×¤×¢×.
- ×× ×”××©×ª××© ×›×•×ª×‘ ××©×¤×˜ ×˜×¢×•×Ÿ ×©×™×¤×•×¨ â†’ ××ª×” ××ª×—×™×œ ×ª×”×œ×™×š BEZEN.
- ×× ×”××©×ª××© ××‘×§×© "×¨×§ ×œ×”×™×¨×’×¢ ×¢×›×©×™×•" â†’ ×ª×Ÿ ×¢×•×’×Ÿ ×§×¦×¨ + × ×©×™××” + ×¤×¢×•×œ×” ××—×ª.
- ×ª××™×“ ×¡×™×™× ×‘"××” ×”×›×™ × ×›×•×Ÿ ×¢×›×©×™×•: (A/B/C)?" ×¢× 3 ××¤×©×¨×•×™×•×ª ×§×¦×¨×•×ª.

--- BRAIN ---
{brain}

--- RULES ---
{rules}

--- WORKFLOWS ---
{workflows}
""".strip()

    print("âœ… BEZEN-CORE ×¢×œ×”. ×›×ª×•×‘ ×”×•×“×¢×” ×•×”×§×© Enter.")
    print("×˜×™×¤: ×›×ª×•×‘ ××©×¤×˜ ×˜×¢×•×Ÿ ×©×™×¤×•×¨ ×›××•: '×× ×× ×™ ×œ× ×§×™×™× ×‘×¨×©×ª â€“ ××£ ××—×“ ×œ× ×™×¨×’×™×© ×‘×—×¡×¨×•× ×™'")

    while True:
        user_text = input("\n××ª×”: ").strip()
        if not user_text:
            continue
        if user_text.lower() in {"exit", "quit", "q"} or user_text in {"×™×¦×™××”", "×¦×"}:
            print("ğŸ‘‹ ×™×¦×™××”.")
            break

        history.append({"role": "user", "content": user_text, "ts": now_iso()})

        # Keep last N turns to stay fast & stable
        recent = history[-20:]
        messages = [{"role": "system", "content": system_prompt}]
        for h in recent:
            messages.append({"role": h["role"], "content": h["content"]})

        resp = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.4,
        )

        answer = resp.choices[0].message.content.strip()
        print("\nBEZEN-CORE:", answer)

        history.append({"role": "assistant", "content": answer, "ts": now_iso()})
        state["history"] = history
        save_state(state)

if __name__ == "__main__":
    main()
